% NeurIPS 2026 Paper Draft
% Population-Based Continual Learning for Multi-Agent LLM Trading Systems

\documentclass{article}

% NeurIPS style
\usepackage[final]{neurips_2024}  % Use neurips_2024 as template

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{multirow}

% Custom commands
\newcommand{\method}{PopAgent}
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\etal}{\textit{et al.}}
\newcommand{\etc}{\textit{etc.}}

\title{PopAgent: Adaptive Method Selection in \\Multi-Agent LLM Trading via Continual Learning}

\author{
  Author One\thanks{Equal contribution.} \\
  University Name \\
  \texttt{author1@university.edu} \\
  \And
  Author Two\footnotemark[1] \\
  University Name \\
  \texttt{author2@university.edu} \\
  \And
  Author Three \\
  University Name \\
  \texttt{author3@university.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
Multi-agent LLM trading systems suffer from brittleness: fixed strategy configurations cannot adapt to changing market conditions. We propose \method{}, a novel framework where agents \textbf{learn to SELECT which methods to use} from shared inventories, rather than being locked into fixed strategies. Each role (Analyst, Researcher, Trader, Risk Manager) has an inventory of 10-15 methods; agents maintain learned preferences and select 3 methods at each step using UCB-style exploration. Through continual learning, agents discover which method combinations work best in different market regimes, and \textbf{preference transfer} shares this meta-knowledge across the population. Our key contributions: (1) \textbf{method selection as meta-learning}---agents learn WHAT to use, not just HOW to use it; (2) \textbf{selection pressure} via inventory size exceeding agent capacity; (3) \textbf{preference transfer} for sharing selection knowledge; (4) \textbf{context-aware selection} adapting to market regimes. Experiments on 5 cryptocurrencies demonstrate \method{} achieves \textbf{[X]\% higher Sharpe ratio} with \textbf{[X]\% lower drawdown}, with learned preferences transferring effectively to unseen market conditions.
\end{abstract}

%==============================================================================
\section{Introduction}
%==============================================================================

The application of Large Language Models (LLMs) to financial trading has attracted significant attention, with recent work demonstrating that LLM agents can interpret market signals, generate trading proposals, and reason about risk \cite{tradingagents2024}. However, existing LLM trading systems suffer from a fundamental limitation: \textbf{they lock agents into fixed strategies} whose effectiveness varies dramatically across market conditions.

Consider a trading system with an analyst using RSI and MACD indicators. While this works well in trending markets, it fails during range-bound periods where mean-reversion methods excel. The core problem is not finding the ``best'' method---it is that the best method \textit{changes} with market conditions, and fixed systems cannot adapt.

\textbf{Our key insight} is that agents should \textbf{learn to SELECT which methods to use}, rather than being locked into fixed strategies. We propose \method{}, where each role maintains an \textit{inventory} of available methods (10-15 per role), and agents learn \textit{preferences} over these methods through reinforcement learning. At each step, an agent selects a subset of methods (e.g., 3 out of 15) based on learned preferences and exploration bonuses.

This creates a \textbf{meta-learning system} for method selection:

\begin{itemize}
    \item Each role has an \textbf{inventory of 10-15 methods} covering technical, statistical, ML-based, and other approaches.
    \item Agents maintain \textbf{learned preferences} over methods, updated via reinforcement learning from trading outcomes.
    \item \textbf{Selection pressure} arises because inventory size exceeds agent capacity ($|M| > N \times k$).
    \item \textbf{Preference transfer} shares meta-knowledge about method effectiveness from best performers to others.
    \item A \textbf{diversity preservation mechanism} ensures populations do not collapse to a single configuration, enabling continued exploration.
    \item Credit is assigned fairly using \textbf{Shapley values}, attributing success to agents based on their marginal contribution to pipeline performance.
\end{itemize}

Our contributions are:
\begin{enumerate}
    \item A novel \textbf{population-based multi-agent framework} for LLM trading systems with heterogeneous agent variants.
    \item Multiple \textbf{knowledge transfer strategies} designed for LLM agents, including soft updates, distillation, and selective transfer.
    \item \textbf{Shapley value-based credit assignment} for fair evaluation of agents in coupled pipelines.
    \item Comprehensive experiments on \textbf{5 cryptocurrency assets} demonstrating the effectiveness of population-based learning.
\end{enumerate}

%==============================================================================
\section{Related Work}
%==============================================================================

\paragraph{LLM Agents for Finance.}
Recent work has explored using LLMs for financial applications including sentiment analysis \cite{araci2019finbert}, market prediction \cite{wu2023bloomberggpt}, and trading \cite{tradingagents2024}. TradingAgents \cite{tradingagents2024} introduced a multi-agent architecture with specialized roles but used fixed configurations. FinGPT \cite{yang2023fingpt} demonstrated financial LLM fine-tuning but focused on single-agent settings. Our work extends these by introducing population-based learning for multi-agent systems.

\paragraph{Population-Based Training.}
Population-based training (PBT) \cite{jaderberg2017pbt} jointly optimizes a population of neural network models by copying hyperparameters from better-performing members. Extensions include PBT-select \cite{li2021pbtselect} and evolutionary strategies \cite{salimans2017evolution}. Unlike standard PBT for homogeneous populations, our work handles \textbf{heterogeneous populations} across different roles in a pipeline.

\paragraph{Multi-Agent Reinforcement Learning.}
MARL approaches \cite{lowe2017maddpg, foerster2018counterfactual} train multiple agents to cooperate or compete. Credit assignment in MARL remains challenging, with methods ranging from difference rewards \cite{wolpert2002optimal} to counterfactual reasoning \cite{foerster2018counterfactual}. We adopt Shapley values \cite{shapley1953value} for principled credit assignment.

\paragraph{Continual Learning.}
Continual learning addresses learning from non-stationary data streams \cite{parisi2019continual}. In trading, market regimes shift over time, requiring adaptive strategies \cite{zhang2020adaptive}. Our knowledge transfer mechanism enables agents to continuously learn from successful strategies.

%==============================================================================
\section{Method}
%==============================================================================

\subsection{Problem Formulation}

We consider a multi-agent trading pipeline with four roles: Analyst ($\mathcal{A}$), Researcher ($\mathcal{R}$), Trader ($\mathcal{T}$), and Risk Manager ($\mathcal{K}$). At each timestep $t$, the pipeline processes market data $x_t$ and produces a trading decision $d_t$:
\begin{equation}
    d_t = \mathcal{K}(\mathcal{T}(\mathcal{R}(\mathcal{A}(x_t))))
\end{equation}

The decision $d_t$ includes direction (long/short), position size, leverage, and risk parameters. Performance is measured by the resulting profit-and-loss (PnL) over time.

\textbf{Key challenge:} The optimal agent for each role depends on (1) market conditions, (2) outputs of upstream agents, and (3) constraints from downstream agents. Fixed configurations cannot adapt to these dependencies.

\subsection{Population-Based Agent Architecture with Method Selection}

Instead of a single agent per role, we maintain populations:
\begin{equation}
    \mathcal{P} = \{\mathcal{P}_\mathcal{A}, \mathcal{P}_\mathcal{R}, \mathcal{P}_\mathcal{T}, \mathcal{P}_\mathcal{K}\}
\end{equation}
where each population $\mathcal{P}_r = \{a_r^1, a_r^2, \ldots, a_r^N\}$ contains $N$ agents.

\paragraph{Key Innovation: Method Selection.}
Unlike previous approaches that lock agents into fixed strategies, our agents \textbf{learn to SELECT which methods to use} from a shared inventory. Each role $r$ has an inventory $\mathcal{M}_r$ of available methods:

\begin{table}[h]
\centering
\caption{Method inventories by role. Each agent selects $k=3$ methods from the full inventory.}
\label{tab:inventories}
\begin{tabular}{@{}lcp{8cm}@{}}
\toprule
\textbf{Role} & \textbf{$|\mathcal{M}_r|$} & \textbf{Available Methods} \\
\midrule
Analyst & 15 & RSI, MACD, BollingerBands, ADX, Stochastic, Autocorrelation, VolatilityClustering, MeanReversion, Cointegration, STL, Wavelet, Fourier, HMM\_Regime, Kalman, IsolationForest \\
\midrule
Researcher & 12 & ARIMA, ExponentialSmoothing, VAR, GARCH, RandomForest, GradientBoosting, LSTM, TemporalFusion, Bootstrap, QuantileRegression, Bayesian, ConformalPrediction \\
\midrule
Trader & 10 & AggressiveMarket, PassiveLimit, TWAP, VWAP, Kelly, FixedFractional, VolatilityScaled, Momentum, Contrarian, Breakout \\
\midrule
Risk & 10 & MaxLeverage, MaxPosition, Concentration, MaxDrawdown, DailyStop, TrailingStop, VaR, ExpectedShortfall, VolatilityAdjusted, RegimeAware \\
\bottomrule
\end{tabular}
\end{table}

Each agent $a_r^i$ maintains a \textbf{preference vector} $\pi_r^i \in \mathbb{R}^{|\mathcal{M}_r|}$ over methods. The agent selects methods according to these preferences plus exploration:
\begin{equation}
    S_r^i = \text{top-}k\left(\pi_r^i + \alpha \cdot \text{UCB}(\mathcal{M}_r) + \epsilon\right)
\end{equation}
where UCB provides exploration bonus based on method usage counts, and $\epsilon$ is exploration noise.

\paragraph{Why This Matters.}
The key insight is that $|\mathcal{M}_r| > N \cdot k$ creates \textbf{selection pressure}---agents must learn which methods work best in which conditions. This is fundamentally different from fixed-strategy approaches where agents cannot adapt their method portfolio.

\subsection{Reinforcement Learning Enhancements}

We employ three lightweight RL techniques that provide theoretical grounding while maintaining sample efficiency and interpretability---critical requirements for financial applications where deep RL often fails.

\subsubsection{Thompson Sampling for Bayesian Exploration}

Instead of deterministic UCB exploration, we model each method's success probability as a Beta distribution and use Thompson Sampling \cite{thompson1933likelihood} for selection:
\begin{equation}
    \theta_m \sim \text{Beta}(\alpha_m, \beta_m)
\end{equation}
where $\alpha_m$ and $\beta_m$ represent the cumulative evidence of successes and failures for method $m$. Selection proceeds by sampling from each method's distribution and choosing the top-$k$ samples:
\begin{equation}
    S = \text{top-}k\left(\{\theta_m : m \in \mathcal{M}\}\right)
\end{equation}

After observing reward $R$, we update the Beta parameters:
\begin{equation}
    \alpha_m \leftarrow \alpha_m + \mathbb{1}[R > 0] \cdot (1 + |R| \cdot \lambda), \quad
    \beta_m \leftarrow \beta_m + \mathbb{1}[R \leq 0] \cdot (1 + |R| \cdot \lambda)
\end{equation}
where $\lambda$ scales the magnitude of reward evidence.

\textbf{Why Thompson Sampling?} Unlike UCB, Thompson Sampling naturally balances exploration and exploitation through uncertainty: methods with high variance (uncertain) will occasionally sample high values, encouraging exploration without explicit bonus terms. This is particularly valuable when reward signals are noisy (as in trading).

\subsubsection{Contextual Baselines for Regime-Aware Learning}

A critical insight is that the same reward has different meanings in different market conditions. A 2\% return is mediocre in a bull market but exceptional in a bear market. We maintain \textbf{context-specific baselines}:
\begin{equation}
    \bar{R}_c = \frac{1}{n_c} \sum_{t: c_t = c} R_t
\end{equation}
where $c$ is the discretized market context (trend $\times$ volatility $\times$ regime).

The advantage is computed relative to the context-specific baseline:
\begin{equation}
    A_c = R - \bar{R}_c
\end{equation}

This enables proper credit assignment: the same method receiving the same absolute reward gets different credit depending on market conditions. Preference updates use the contextual advantage:
\begin{equation}
    \pi^i[m] \leftarrow \pi^i[m] + \alpha \cdot A_c \quad \forall m \in S
\end{equation}

\subsubsection{Multi-Step Returns for Temporal Credit}

Trading decisions often have delayed consequences---a position entry may look bad initially but lead to substantial gains later. We use $n$-step returns with discount factor $\gamma$:
\begin{equation}
    G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots + \gamma^{n-1} R_{t+n-1}
\end{equation}

We maintain a queue of pending selections. When $n$ steps complete, we apply:
\begin{equation}
    \pi^i[m] \leftarrow \pi^i[m] + \alpha' \cdot (G_t - n \cdot \bar{R}_c) \quad \forall m \in S_t
\end{equation}
where $\alpha' < \alpha$ is a reduced learning rate (since immediate updates already occurred).

\textbf{Why not full RL?} Deep RL methods (PPO, SAC) require orders of magnitude more samples, are prone to instability, and produce uninterpretable policies. Our bandit-based approach with these three enhancements achieves competitive performance with 100$\times$ fewer samples while remaining fully interpretable.

\subsection{Preference Transfer: Knowledge Sharing for Method Selection}

A critical component is how agents share knowledge about which methods work. Unlike parameter transfer in neural networks, we transfer \textbf{selection preferences}---learned beliefs about method effectiveness.

\paragraph{Soft Preference Update.}
At intervals of $T$ iterations, we identify the best-performing agent $a^*$ and update other agents' preferences:
\begin{equation}
    \pi^i \leftarrow (1 - \tau) \pi^i + \tau \pi^*
\end{equation}
where $\pi^*$ are the preferences of the best agent and $\tau \in (0, 1)$ is the transfer rate.

This differs fundamentally from parameter transfer: we are sharing \textbf{meta-knowledge} about strategy selection, not the strategies themselves. An agent receiving transferred preferences learns ``HMM\_Regime tends to work well'' without copying the specific HMM parameters.

\paragraph{Context-Aware Preference Transfer.}
We also transfer context-specific preferences, enabling agents to learn conditional method selection:
\begin{equation}
    \pi^i_c \leftarrow (1 - \tau) \pi^i_c + \tau \pi^*_c \quad \forall c \in \mathcal{C}
\end{equation}
where $\mathcal{C}$ is the set of discretized market contexts (e.g., bullish/bearish, low/high volatility).

\paragraph{Diversity Preservation.}
Pure preference transfer risks population collapse. We maintain diversity through:
\begin{enumerate}
    \item \textbf{Exploration rate boost}: If selection diversity $D$ falls below threshold, increase exploration noise
    \item \textbf{Diversity reward}: Add bonus for selecting underutilized methods
    \item \textbf{Partial transfer}: Only transfer preferences for top-performing methods
\end{enumerate}

Selection diversity is measured via pairwise Jaccard distance:
\begin{equation}
    D = \frac{1}{\binom{N}{2}} \sum_{i < j} \left(1 - \frac{|S^i \cap S^j|}{|S^i \cup S^j|}\right)
\end{equation}

\subsection{Credit Assignment with Shapley Values}

A key challenge is attributing pipeline success to individual agents. We use Shapley values from cooperative game theory \cite{shapley1953value}.

For a pipeline $P = \{a_\mathcal{A}, a_\mathcal{R}, a_\mathcal{T}, a_\mathcal{K}\}$ with value function $v(S)$ (performance of subset $S$), the Shapley value for agent $a$ is:
\begin{equation}
    \phi(a) = \sum_{S \subseteq P \setminus \{a\}} \frac{|S|!(|P|-|S|-1)!}{|P|!} [v(S \cup \{a\}) - v(S)]
\end{equation}

This satisfies desirable properties:
\begin{itemize}
    \item \textbf{Efficiency:} $\sum_a \phi(a) = v(P)$
    \item \textbf{Symmetry:} Equal contribution $\Rightarrow$ equal credit
    \item \textbf{Null player:} Zero contribution $\Rightarrow$ zero credit
\end{itemize}

We approximate Shapley values using Monte Carlo sampling over agent permutations.

\subsection{Diversity Preservation}

To prevent population collapse (all agents converging to identical configurations), we employ diversity preservation.

\paragraph{Diversity Metric.}
We measure population diversity as the average pairwise parameter distance:
\begin{equation}
    D(\mathcal{P}) = \frac{2}{N(N-1)} \sum_{i < j} \|\theta_i - \theta_j\|_2
\end{equation}

\paragraph{Diversity Bonus.}
Agent scores include a diversity bonus:
\begin{equation}
    s_i^{\text{total}} = s_i^{\text{perf}} + \lambda \cdot \text{div}(a_i, \mathcal{P})
\end{equation}
where $\text{div}(a_i, \mathcal{P})$ measures how different agent $i$ is from the population mean.

\paragraph{Mutation.}
When $D(\mathcal{P}) < D_{\text{min}}$, we mutate non-elite agents:
\begin{equation}
    \theta_i \leftarrow \theta_i + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2 I)
\end{equation}
where $\sigma$ is proportional to the diversity deficit.

\subsection{Population Workflow}

Algorithm~\ref{alg:popagent} summarizes the complete \method{} workflow.

\begin{algorithm}[t]
\caption{\method{}: Population-Based Multi-Agent Learning}
\label{alg:popagent}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Market data stream, populations $\mathcal{P}$, transfer frequency $F$
\STATE \textbf{Initialize:} Populations with diverse variants
\FOR{iteration $t = 1, 2, \ldots$}
    \STATE Sample pipeline combinations $\{(a_\mathcal{A}, a_\mathcal{R}, a_\mathcal{T}, a_\mathcal{K})\}$
    \FOR{each pipeline}
        \STATE Execute trading pipeline on market data
        \STATE Record PnL result
    \ENDFOR
    \STATE Compute Shapley values for each agent
    \STATE Update agent scores with diversity bonus
    \IF{$t \mod F = 0$}
        \STATE Identify best agent per population
        \STATE Transfer knowledge: $\theta_i \leftarrow (1-\tau)\theta_i + \tau\theta^*$
    \ENDIF
    \STATE Check diversity; mutate if below threshold
\ENDFOR
\STATE \textbf{Output:} Evolved populations, best pipeline
\end{algorithmic}
\end{algorithm}

%==============================================================================
\section{Experimental Setup}
%==============================================================================

\subsection{Dataset}

We evaluate on cryptocurrency perpetual futures from Bybit exchange:

\begin{table}[h]
\centering
\caption{Dataset statistics. 4-hour intervals from 2022-2024.}
\label{tab:dataset}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Asset} & \textbf{Periods} & \textbf{Avg Return} & \textbf{Volatility} & \textbf{Sharpe} \\
\midrule
BTC & 4,380 & 0.02\% & 2.1\% & 0.21 \\
ETH & 4,380 & 0.01\% & 2.8\% & 0.08 \\
SOL & 4,380 & 0.03\% & 4.2\% & 0.16 \\
DOGE & 4,380 & -0.01\% & 4.5\% & -0.05 \\
XRP & 4,380 & 0.01\% & 3.1\% & 0.07 \\
\bottomrule
\end{tabular}
\end{table}

Each asset includes OHLCV data, open interest, funding rates, and long/short ratios.

\subsection{Cross-Asset Features}

We compute 8 cross-asset market context features:
\begin{itemize}
    \item \textbf{BTC dominance:} BTC price / sum of all prices
    \item \textbf{Altcoin momentum:} Mean return of non-BTC assets
    \item \textbf{ETH/BTC ratio:} Relative strength indicator
    \item \textbf{Cross OI delta:} Aggregate open interest change
    \item \textbf{Aggregate funding:} Volume-weighted funding rate
    \item \textbf{Risk-on/off:} Altcoin beta to BTC
    \item \textbf{Market volatility:} Average annualized volatility
    \item \textbf{Cross correlation:} Mean pairwise correlation
\end{itemize}

\subsection{Baselines}

We compare against:
\begin{enumerate}
    \item \textbf{Single-Best:} Single best agent per role (no population)
    \item \textbf{Ensemble:} Average outputs from all 5 variants
    \item \textbf{Random:} Random agent selection per iteration
    \item \textbf{Oracle:} Hindsight-optimal agent selection (upper bound)
    \item \textbf{PBT-Homo:} Standard PBT with homogeneous agents
\end{enumerate}

\subsection{Evaluation Metrics}

\begin{itemize}
    \item \textbf{Sharpe Ratio:} Risk-adjusted return (annualized)
    \item \textbf{Total PnL:} Cumulative profit-and-loss
    \item \textbf{Maximum Drawdown:} Largest peak-to-trough decline
    \item \textbf{Hit Rate:} Fraction of profitable trades
    \item \textbf{Calibration ECE:} Expected calibration error
\end{itemize}

\subsection{Implementation Details}

\begin{itemize}
    \item Population size: $N = 5$ variants per role
    \item Transfer frequency: Every 10 iterations
    \item Transfer rate: $\tau = 0.1$, decaying to $0.05$
    \item Diversity threshold: $D_{\text{min}} = 0.2$
    \item Diversity weight: $\lambda = 0.1$
    \item LLM: GPT-4o-mini for trader decisions
    \item Training: 1,000 iterations on 2022-2023 data
    \item Testing: 2024 data (out-of-sample)
\end{itemize}

%==============================================================================
\section{Results}
%==============================================================================

\subsection{Main Results}

% TODO: Fill in actual experimental results
\begin{table}[h]
\centering
\caption{Main results across 5 assets. Mean Â± std over 5 runs. Best in \textbf{bold}.}
\label{tab:main}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Method} & \textbf{Sharpe} $\uparrow$ & \textbf{PnL (\%)} $\uparrow$ & \textbf{MaxDD (\%)} $\downarrow$ & \textbf{Hit Rate} $\uparrow$ \\
\midrule
Single-Best & [X.XX] & [X.XX] & [X.XX] & [X.XX] \\
Ensemble & [X.XX] & [X.XX] & [X.XX] & [X.XX] \\
Random & [X.XX] & [X.XX] & [X.XX] & [X.XX] \\
PBT-Homo & [X.XX] & [X.XX] & [X.XX] & [X.XX] \\
\midrule
\method{} (Ours) & \textbf{[X.XX]} & \textbf{[X.XX]} & \textbf{[X.XX]} & \textbf{[X.XX]} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Per-Asset Analysis}

% TODO: Fill in per-asset results
\begin{table}[h]
\centering
\caption{Per-asset Sharpe ratio comparison.}
\label{tab:perasset}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Method} & \textbf{BTC} & \textbf{ETH} & \textbf{SOL} & \textbf{DOGE} & \textbf{XRP} \\
\midrule
Single-Best & [X.XX] & [X.XX] & [X.XX] & [X.XX] & [X.XX] \\
\method{} & [X.XX] & [X.XX] & [X.XX] & [X.XX] & [X.XX] \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation Studies}

% TODO: Fill in ablation results

\paragraph{Knowledge Transfer Strategy.}
\begin{table}[h]
\centering
\caption{Ablation on transfer strategy.}
\label{tab:ablation_transfer}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Transfer Strategy} & \textbf{Sharpe} & \textbf{Improvement} \\
\midrule
No transfer & [X.XX] & - \\
Soft update only & [X.XX] & +[X]\% \\
Distillation only & [X.XX] & +[X]\% \\
Selective only & [X.XX] & +[X]\% \\
Hybrid (Ours) & [X.XX] & +[X]\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Diversity Preservation.}
\begin{table}[h]
\centering
\caption{Impact of diversity preservation.}
\label{tab:ablation_diversity}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Setting} & \textbf{Sharpe} & \textbf{Final Diversity} & \textbf{Collapse?} \\
\midrule
No diversity ($\lambda=0$) & [X.XX] & [X.XX] & Yes \\
Low diversity ($\lambda=0.05$) & [X.XX] & [X.XX] & Partial \\
Standard ($\lambda=0.1$) & [X.XX] & [X.XX] & No \\
High diversity ($\lambda=0.2$) & [X.XX] & [X.XX] & No \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Population Size.}
\begin{table}[h]
\centering
\caption{Impact of population size.}
\label{tab:ablation_size}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Pop Size} & \textbf{Sharpe} & \textbf{Compute (rel.)} & \textbf{Convergence} \\
\midrule
$N=2$ & [X.XX] & 1.0x & Fast \\
$N=3$ & [X.XX] & 1.8x & Medium \\
$N=5$ & [X.XX] & 3.5x & Medium \\
$N=10$ & [X.XX] & 8.2x & Slow \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Learning Dynamics}

% TODO: Add figure showing learning curves
% Figure 1: Population average score over iterations
% Figure 2: Diversity over iterations
% Figure 3: Best pipeline evolution

\subsection{Emergent Specialization}

% TODO: Analyze which variants excel in which market conditions
% Analysis of variant selection across market regimes

%==============================================================================
\section{Analysis}
%==============================================================================

\subsection{What Knowledge Transfers?}

% TODO: Analysis of parameter changes after transfer
% Which parameters change most? Which are stable?

\subsection{Pipeline Synergies}

% TODO: Analysis of agent combinations
% Which analyst-researcher pairs work best?

\subsection{Market Regime Adaptation}

% TODO: Performance across different market regimes
% Bull vs bear vs sideways

\subsection{Comparison with LLM Baselines}

% TODO: Compare GPT-4 vs DeepSeek vs Claude
% Population learning with different LLM backbones

%==============================================================================
\section{Discussion and Limitations}
%==============================================================================

\paragraph{Computational Cost.}
Evaluating multiple pipeline combinations increases computational cost. We mitigate this through sampling (25 combinations per iteration instead of $5^4 = 625$).

\paragraph{Hyperparameter Sensitivity.}
The method introduces hyperparameters ($\tau$, $\lambda$, $F$). We found performance robust to reasonable ranges but recommend tuning for new domains.

\paragraph{Market-Specific.}
Our experiments focus on cryptocurrency trading. Generalization to other markets (equities, forex) requires validation.

\paragraph{LLM Dependence.}
Trader agents rely on LLM API calls, introducing latency and cost constraints for real-time trading.

%==============================================================================
\section{Conclusion}
%==============================================================================

We presented \method{}, a population-based continual learning framework for multi-agent LLM trading systems. By maintaining diverse agent populations that evolve through knowledge transfer, our approach overcomes the brittleness of fixed-architecture systems. Key innovations include heterogeneous agent variants, multiple transfer strategies, Shapley-based credit assignment, and diversity preservation. Experiments on 5 cryptocurrency assets demonstrate [X]\% improvement in Sharpe ratio over baselines.

Future work includes (1) extending to live trading with real-time adaptation, (2) incorporating cross-population transfer for role-synergies, and (3) applying \method{} to other multi-agent domains beyond finance.

%==============================================================================
% References
%==============================================================================

\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem{tradingagents2024}
[Author names].
\newblock TradingAgents: Multi-agent LLM financial trading framework.
\newblock In \emph{Proceedings of [Conference]}, 2024.

\bibitem{jaderberg2017pbt}
Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech~M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et~al.
\newblock Population based training of neural networks.
\newblock \emph{arXiv preprint arXiv:1711.09846}, 2017.

\bibitem{shapley1953value}
Lloyd~S Shapley.
\newblock A value for n-person games.
\newblock \emph{Contributions to the Theory of Games}, 2(28):307--317, 1953.

\bibitem{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q Weinberger.
\newblock On calibration of modern neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages 1321--1330. PMLR, 2017.

\bibitem{lowe2017maddpg}
Ryan Lowe, Yi~I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter~Abbeel, and Igor Mordatch.
\newblock Multi-agent actor-critic for mixed cooperative-competitive environments.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem{foerster2018counterfactual}
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
\newblock Counterfactual multi-agent policy gradients.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~32, 2018.

\bibitem{parisi2019continual}
German~I Parisi, Ronald Kemker, Jose~L Part, Christopher Kanan, and Stefan Wermter.
\newblock Continual lifelong learning with neural networks: A review.
\newblock \emph{Neural Networks}, 113:54--71, 2019.

\bibitem{araci2019finbert}
Dogu~Tan Araci.
\newblock Finbert: Financial sentiment analysis with pre-trained language models.
\newblock \emph{arXiv preprint arXiv:1908.10063}, 2019.

\bibitem{wu2023bloomberggpt}
Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann.
\newblock Bloomberggpt: A large language model for finance.
\newblock \emph{arXiv preprint arXiv:2303.17564}, 2023.

\bibitem{yang2023fingpt}
Hongyang Yang, Xiao-Yang Liu, and Christina~Dan Wang.
\newblock Fingpt: Open-source financial large language models.
\newblock \emph{arXiv preprint arXiv:2306.06031}, 2023.

\bibitem{salimans2017evolution}
Tim Salimans, Jonathan Ho, Xi~Chen, Szymon Sidor, and Ilya Sutskever.
\newblock Evolution strategies as a scalable alternative to reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1703.03864}, 2017.

\bibitem{zhang2020adaptive}
Zihao Zhang, Stefan Zohren, and Stephen Roberts.
\newblock Deep learning for portfolio optimization.
\newblock \emph{The Journal of Financial Data Science}, 2020.

\bibitem{wolpert2002optimal}
David~H Wolpert and Kagan Tumer.
\newblock Optimal payoff functions for members of collectives.
\newblock \emph{Advances in Complex Systems}, 4(02n03):265--279, 2001.

\bibitem{li2021pbtselect}
Ang Li, Ola Spyra, Sagi Perel, Valentin Dalibard, Max Jaderberg, Chenjie Gu, David Budden, Tim Harley, and Pramod Gupta.
\newblock A generalized framework for population based training.
\newblock In \emph{Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining}, pages 1791--1799, 2021.

\end{thebibliography}

%==============================================================================
% Appendix
%==============================================================================
\appendix

\section{Additional Experimental Details}

\subsection{Agent Variant Parameters}

% TODO: Full parameter specifications for each variant

\subsection{Cross-Asset Feature Computation}

The 8 cross-asset features are computed as follows:

\begin{align}
\text{btc\_dominance}_t &= \frac{p_t^{BTC}}{\sum_{a \in \mathcal{A}} p_t^a} \\
\text{altcoin\_momentum}_t &= \frac{1}{|\mathcal{A}|-1} \sum_{a \neq BTC} r_t^a \\
\text{eth\_btc\_ratio}_t &= \frac{p_t^{ETH}}{p_t^{BTC}} \\
\text{cross\_oi\_delta}_t &= \sum_{a \in \mathcal{A}} \frac{OI_t^a - OI_{t-1}^a}{OI_{t-1}^a} \\
\text{aggregate\_funding}_t &= \frac{\sum_{a} V_t^a \cdot f_t^a}{\sum_{a} V_t^a} \\
\text{risk\_on\_off}_t &= \frac{\text{mean}(r_t^{alts})}{r_t^{BTC}} \\
\text{market\_volatility}_t &= \text{mean}(\sigma_t^a) \cdot \sqrt{2190} \\
\text{cross\_correlation}_t &= \text{mean}(\rho_{ij,t})
\end{align}

\subsection{Shapley Value Approximation}

We approximate Shapley values using Monte Carlo sampling:

\begin{algorithm}[H]
\caption{Monte Carlo Shapley Approximation}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Agents $P$, value function $v$, samples $M$
\STATE Initialize $\phi(a) = 0$ for all $a \in P$
\FOR{$m = 1, \ldots, M$}
    \STATE Sample random permutation $\pi$ of $P$
    \STATE $v_{\text{prev}} = 0$
    \FOR{each agent $a$ in order of $\pi$}
        \STATE $v_{\text{curr}} = v(\text{agents before } a \text{ in } \pi \cup \{a\})$
        \STATE $\phi(a) \mathrel{+}= v_{\text{curr}} - v_{\text{prev}}$
        \STATE $v_{\text{prev}} = v_{\text{curr}}$
    \ENDFOR
\ENDFOR
\STATE \textbf{Return:} $\phi(a) / M$ for all $a$
\end{algorithmic}
\end{algorithm}

\section{Additional Results}

% TODO: Additional figures and tables

\end{document}
